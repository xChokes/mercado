#!/usr/bin/env python3
"""
Ejemplo de uso del sistema de calibraci√≥n autom√°tica
Demuestra diferentes casos de uso y an√°lisis de resultados
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path

# Agregar el directorio ra√≠z al path
current_dir = Path(__file__).parent
root_dir = current_dir.parent
sys.path.append(str(root_dir))

from src.utils.calibration_runner import CalibrationRunner, CalibrationConfig


def ejemplo_calibracion_basica():
    """Ejemplo 1: Calibraci√≥n econ√≥mica b√°sica"""
    print("=" * 60)
    print("EJEMPLO 1: CALIBRACI√ìN ECON√ìMICA B√ÅSICA")
    print("=" * 60)
    
    # Configuraci√≥n simple para calibraci√≥n b√°sica
    config = CalibrationConfig(
        parameters={
            'pib_inicial': (80000, 120000),
            'tasa_inflacion_objetivo': (0.015, 0.030),
            'tasa_desempleo_inicial': (0.08, 0.15),
        },
        optimization_method="bayesian",
        n_trials=10,  # Pocos trials para demo r√°pida
        simulation_cycles=8,
        target_metrics={
            'pib_final': (100000.0, 1.0),
            'inflacion_promedio': (0.02, 2.0),
            'desempleo_promedio': (0.06, 1.5),
        },
        results_dir="results/calibrations/ejemplo1"
    )
    
    print(f"Configuraci√≥n:")
    print(f"- M√©todo: {config.optimization_method}")
    print(f"- Trials: {config.n_trials}")
    print(f"- Ciclos por simulaci√≥n: {config.simulation_cycles}")
    print(f"- Par√°metros: {list(config.parameters.keys())}")
    
    # Ejecutar calibraci√≥n
    runner = CalibrationRunner(config)
    results = runner.run_calibration()
    
    # Mostrar mejores resultados
    best_result = max(results, key=lambda r: r.score)
    print(f"\nüèÜ MEJOR RESULTADO:")
    print(f"Score: {best_result.score:.4f}")
    print(f"PIB Final: ${best_result.metrics.get('pib_final', 0):,.0f}")
    print(f"Inflaci√≥n: {best_result.metrics.get('inflacion_promedio', 0):.3f}")
    print(f"Desempleo: {best_result.metrics.get('desempleo_promedio', 0):.3f}")
    
    return results


def ejemplo_analisis_sensibilidad():
    """Ejemplo 2: An√°lisis de sensibilidad de precios"""
    print("\n" + "=" * 60)
    print("EJEMPLO 2: AN√ÅLISIS DE SENSIBILIDAD DE PRECIOS")
    print("=" * 60)
    
    # Configuraci√≥n enfocada en par√°metros de precios
    config = CalibrationConfig(
        parameters={
            'ajuste_maximo_por_ciclo': (0.05, 0.25),
            'sensibilidad_competencia': (0.05, 0.20),
        },
        optimization_method="grid",
        n_trials=25,  # 5x5 grid
        simulation_cycles=5,  # R√°pido para demo
        target_metrics={
            'inflacion_promedio': (0.02, 3.0),  # Peso alto en inflaci√≥n
            'volatilidad_inflacion': (0.01, 1.0),
        },
        results_dir="results/calibrations/ejemplo2"
    )
    
    print(f"Configuraci√≥n:")
    print(f"- M√©todo: {config.optimization_method}")
    print(f"- Grid: 5x5 = 25 combinaciones")
    print(f"- Enfoque: Estabilidad de precios")
    
    # Ejecutar calibraci√≥n
    runner = CalibrationRunner(config)
    results = runner.run_calibration()
    
    # An√°lisis de sensibilidad
    df_results = pd.DataFrame([
        {**r.parameters, **r.metrics, 'score': r.score} 
        for r in results
    ])
    
    print(f"\nüìä AN√ÅLISIS DE SENSIBILIDAD:")
    print(f"Rango scores: {df_results['score'].min():.4f} - {df_results['score'].max():.4f}")
    
    # Correlaciones
    correlations = df_results.corr()['score'].sort_values(ascending=False)
    print(f"\nCorrelaciones con score:")
    for param, corr in correlations.items():
        if param != 'score' and abs(corr) > 0.1:
            print(f"  {param}: {corr:.3f}")
    
    return results


def ejemplo_configuracion_personalizada():
    """Ejemplo 3: Configuraci√≥n personalizada completa"""
    print("\n" + "=" * 60)
    print("EJEMPLO 3: CONFIGURACI√ìN PERSONALIZADA")
    print("=" * 60)
    
    # Configuraci√≥n avanzada con muchos par√°metros
    config = CalibrationConfig(
        parameters={
            'pib_inicial': (85000, 115000),
            'tasa_inflacion_objetivo': (0.018, 0.025),
            'tasa_desempleo_inicial': (0.06, 0.12),
            'ajuste_maximo_por_ciclo': (0.08, 0.20),
            'probabilidad_contratacion_base': (0.3, 0.6),
            'tasa_interes_base': (0.02, 0.06),
        },
        optimization_method="bayesian",
        n_trials=15,
        simulation_cycles=10,
        target_metrics={
            'pib_final': (100000.0, 1.5),      # Peso alto PIB
            'inflacion_promedio': (0.02, 2.0), # Peso alto inflaci√≥n
            'desempleo_promedio': (0.08, 1.0), # Tolerancia mayor
            'crecimiento_pib': (0.02, 1.0),    # Crecimiento positivo
            'transacciones_totales': (50.0, 0.5), # Actividad econ√≥mica
        },
        results_dir="results/calibrations/ejemplo3"
    )
    
    print(f"Configuraci√≥n:")
    print(f"- Par√°metros: {len(config.parameters)}")
    print(f"- M√©tricas objetivo: {len(config.target_metrics)}")
    print(f"- Trials: {config.n_trials}")
    
    # Ejecutar calibraci√≥n
    runner = CalibrationRunner(config)
    results = runner.run_calibration()
    
    # An√°lisis multi-objetivo
    print(f"\nüéØ AN√ÅLISIS MULTI-OBJETIVO:")
    
    # Top 3 resultados
    top_results = sorted(results, key=lambda r: r.score, reverse=True)[:3]
    
    for i, result in enumerate(top_results, 1):
        print(f"\nTop {i} (Score: {result.score:.4f}):")
        print(f"  PIB: ${result.metrics.get('pib_final', 0):,.0f}")
        print(f"  Inflaci√≥n: {result.metrics.get('inflacion_promedio', 0):.3f}")
        print(f"  Desempleo: {result.metrics.get('desempleo_promedio', 0):.3f}")
        print(f"  Crecimiento: {result.metrics.get('crecimiento_pib', 0):.3f}")
    
    return results


def ejemplo_comparacion_metodos():
    """Ejemplo 4: Comparaci√≥n entre m√©todos de optimizaci√≥n"""
    print("\n" + "=" * 60)
    print("EJEMPLO 4: COMPARACI√ìN BAYESIANO vs GRID")
    print("=" * 60)
    
    # Configuraci√≥n base com√∫n
    base_params = {
        'pib_inicial': (90000, 110000),
        'tasa_inflacion_objetivo': (0.018, 0.022),
    }
    
    base_metrics = {
        'pib_final': (100000.0, 1.0),
        'inflacion_promedio': (0.02, 2.0),
    }
    
    # Bayesiano
    print("Ejecutando optimizaci√≥n Bayesiana...")
    config_bayes = CalibrationConfig(
        parameters=base_params,
        optimization_method="bayesian",
        n_trials=8,
        simulation_cycles=5,
        target_metrics=base_metrics,
        results_dir="results/calibrations/bayes_comparison"
    )
    
    runner_bayes = CalibrationRunner(config_bayes)
    results_bayes = runner_bayes.run_calibration()
    best_bayes = max(results_bayes, key=lambda r: r.score)
    
    # Grid search
    print("Ejecutando grid search...")
    config_grid = CalibrationConfig(
        parameters=base_params,
        optimization_method="grid",
        n_trials=16,  # 4x4 grid
        simulation_cycles=5,
        target_metrics=base_metrics,
        results_dir="results/calibrations/grid_comparison"
    )
    
    runner_grid = CalibrationRunner(config_grid)
    results_grid = runner_grid.run_calibration()
    best_grid = max(results_grid, key=lambda r: r.score)
    
    # Comparaci√≥n
    print(f"\nüìà COMPARACI√ìN DE RESULTADOS:")
    print(f"Bayesiano - Mejor score: {best_bayes.score:.4f}")
    print(f"  PIB: ${best_bayes.metrics.get('pib_final', 0):,.0f}")
    print(f"  Inflaci√≥n: {best_bayes.metrics.get('inflacion_promedio', 0):.4f}")
    
    print(f"\nGrid Search - Mejor score: {best_grid.score:.4f}")
    print(f"  PIB: ${best_grid.metrics.get('pib_final', 0):,.0f}")
    print(f"  Inflaci√≥n: {best_grid.metrics.get('inflacion_promedio', 0):.4f}")
    
    if best_bayes.score > best_grid.score:
        print("\nüèÜ Bayesiano obtuvo mejor resultado")
    elif best_grid.score > best_bayes.score:
        print("\nüèÜ Grid Search obtuvo mejor resultado")
    else:
        print("\nü§ù Ambos m√©todos obtuvieron resultados similares")
    
    return results_bayes, results_grid


def generar_reporte_completo(todos_los_resultados):
    """Genera un reporte consolidado de todos los ejemplos"""
    print("\n" + "=" * 60)
    print("REPORTE CONSOLIDADO DE CALIBRACI√ìN")
    print("=" * 60)
    
    # Consolidar todos los resultados
    all_results = []
    for ejemplo, results in todos_los_resultados.items():
        if isinstance(results, tuple):  # Comparaci√≥n m√©todos
            results = results[0] + results[1]  # Combinar ambos
        
        for result in results:
            result_dict = {
                'ejemplo': ejemplo,
                'score': result.score,
                'pib_final': result.metrics.get('pib_final', 0),
                'inflacion_promedio': result.metrics.get('inflacion_promedio', 0),
                'desempleo_promedio': result.metrics.get('desempleo_promedio', 0),
                'execution_time': result.execution_time
            }
            all_results.append(result_dict)
    
    df_consolidado = pd.DataFrame(all_results)
    
    print(f"Total de trials ejecutados: {len(df_consolidado)}")
    print(f"Score promedio: {df_consolidado['score'].mean():.4f}")
    print(f"Mejor score global: {df_consolidado['score'].max():.4f}")
    print(f"Tiempo total de simulaci√≥n: {df_consolidado['execution_time'].sum():.1f}s")
    
    # Mejor resultado por ejemplo
    print(f"\nMejores resultados por ejemplo:")
    for ejemplo in df_consolidado['ejemplo'].unique():
        ejemplo_data = df_consolidado[df_consolidado['ejemplo'] == ejemplo]
        best_idx = ejemplo_data['score'].idxmax()
        best = ejemplo_data.loc[best_idx]
        print(f"  {ejemplo}: Score {best['score']:.4f} | PIB ${best['pib_final']:,.0f}")
    
    # Guardar reporte consolidado
    reporte_path = "results/calibrations/reporte_consolidado.csv"
    os.makedirs(os.path.dirname(reporte_path), exist_ok=True)
    df_consolidado.to_csv(reporte_path, index=False)
    print(f"\nüìÅ Reporte guardado en: {reporte_path}")


def main():
    """Funci√≥n principal - ejecuta todos los ejemplos"""
    print("üöÄ SISTEMA DE CALIBRACI√ìN AUTOM√ÅTICA - EJEMPLOS DE USO")
    print("=" * 80)
    
    # Crear directorio base de resultados
    os.makedirs("results/calibrations", exist_ok=True)
    
    # Ejecutar ejemplos
    resultados = {}
    
    try:
        resultados['basica'] = ejemplo_calibracion_basica()
    except Exception as e:
        print(f"‚ùå Error en calibraci√≥n b√°sica: {e}")
        resultados['basica'] = []
    
    try:
        resultados['sensibilidad'] = ejemplo_analisis_sensibilidad()
    except Exception as e:
        print(f"‚ùå Error en an√°lisis de sensibilidad: {e}")
        resultados['sensibilidad'] = []
    
    try:
        resultados['personalizada'] = ejemplo_configuracion_personalizada()
    except Exception as e:
        print(f"‚ùå Error en configuraci√≥n personalizada: {e}")
        resultados['personalizada'] = []
    
    try:
        resultados['comparacion'] = ejemplo_comparacion_metodos()
    except Exception as e:
        print(f"‚ùå Error en comparaci√≥n de m√©todos: {e}")
        resultados['comparacion'] = ([], [])
    
    # Generar reporte final
    try:
        generar_reporte_completo(resultados)
    except Exception as e:
        print(f"‚ùå Error generando reporte: {e}")
    
    print("\n‚úÖ EJEMPLOS COMPLETADOS")
    print("\nPara m√°s informaci√≥n consultar:")
    print("- docs/CALIBRACION_GUIA.md")
    print("- results/calibrations/ (archivos de resultados)")


if __name__ == "__main__":
    main()